{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DeepAR to Forecast Electricity Time Series Data <a name=\"toc\"></a>\n",
    "\n",
    "**Estimated time: 35 min**\n",
    "\n",
    "DeepAR is a supervised learning algorithm for forecasting scalar time series. In this notebook, you will use DeepAR to forecast the electricity demand (across different dimensions) based on a time series of past consumption. \n",
    "\n",
    "This notebook demonstrates how to prepare a time series dataset for training DeepAR and how to use the trained model for inference. This content was designed to run from within an Amazon SageMaker `ml.m4.xlarge` instance.\n",
    "\n",
    "**Table of Contents:** \n",
    "\n",
    "1. [Getting Started](#start)\n",
    "1. [Real Data Set : Electricity Consumption](#data)\n",
    "    1. [Partition the Data into Training and Test Sets](#split)\n",
    "    1. [Visualize the Time Series](#visualize)\n",
    "    1. [Amazon S3 Data Formatting](#s3)\n",
    "1. [DeepAR: LSTM & Monte Carlo](#deepar)\n",
    "    1. [Train the Model](#fit)\n",
    "    1. [Create Endpoint and Predictor](#endpoint)\n",
    "1. [Make Predictions and Plot Results](#predict)\n",
    "1. [Delete Endpoint](#delete)\n",
    "\n",
    "# Getting Started <a name=\"start\"></a>\n",
    "\n",
    "To complete this lab, carefully move through this notebook, from top to bottom, making sure to read all text instructions/explanations and run each code cell in order. Also be sure to view the code cell outputs. To run each cell, step-by-step in the Jupyter notebook, click within the cell and press **SHIFT + ENTER** or choose **Run** at the top of the page. You will know that a code cell has completed running when you see a number inside the square brackets located to the left of the code cell. Alternatively, **[ ]** indicates the cell has yet to be run, and **[*]** indicates that the cell is still processing.\n",
    "\n",
    "Get started with this lab by installing the necessary libraries below.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "whole_start = time.time()\n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Amazon Sagemaker client library for easy interface with the AWS service and `s3fs` for uploading the training data to Amazon S3.\n",
    "\n",
    "(As a side note, not directly related to this lab, you could use `pip` to install missing libraries when necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by specifying:\n",
    "- The Amazon S3 bucket and prefix that you want to use for the training and model data. This should be within the same region as the notebook, training, and hosting instances. Replace `'FILL_IN_LAB_BUCKET_NAME'` below with the name of the  lab's s3 bucket; this can be found in Qwiklabs along the left-hand side, under *LabBucket*. Just copy and paste that bucket name into the appropriate location in the code cell below before running the cell. \n",
    "- The IAM role ARN is used to give training and hosting access to your data. See the documentation for how to create these. Here, the `get_execution_role` function obtains the role ARN, which was specified when creating the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Student needs to set bucket to match the lab S3 bucket name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-80ebc352321f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'FILL_IN_LAB_BUCKET_NAME'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'demos/deepar/forecast-electricity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'FILL_IN_LAB_BUCKET_NAME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Student needs to set bucket to match the lab S3 bucket name.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Student needs to set bucket to match the lab S3 bucket name."
     ]
    }
   ],
   "source": [
    "bucket = 'FILL_IN_LAB_BUCKET_NAME'\n",
    "prefix = 'demos/deepar/forecast-electricity'\n",
    "assert bucket!='FILL_IN_LAB_BUCKET_NAME', 'Student needs to set bucket to match the lab S3 bucket name.'\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)\n",
    "print('Data location: %s'%s3_data_path)\n",
    "print('Output location: %s'%s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data Set: Electricity Consumption <a name=\"data\"></a>\n",
    "\n",
    "**Dataset License and Information:**\n",
    "\n",
    "For this lab, you will be using an open source dataset entitled, [“Individual Household Electric Power Consumption”](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption) that comes from the UCI Machine Learning Repository. Information about the dataset license can be found below. \n",
    "\n",
    "The MIT License (MIT) Copyright © [2017] Zalando SE, [https://tech.zalando.com](https://tech.zalando.com). \n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \n",
    "\n",
    "This archive contains 2,075,259 measurements gathered between December 2006 and November 2010 (47 months). \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "1. (Global_active_power*1000/60 - Sub_metering_1 - Sub_metering_2 - Sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3. \n",
    "2. The dataset contains some missing values in the measurements (nearly 1.25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "1. `Date`: Date in format dd/mm/yyyy \n",
    "2. `Time`: Time in format hh:mm:ss \n",
    "3. `Global_active_power`: Household global minute-averaged active power (in kilowatt) \n",
    "4. `Global_reactive_power`: Household global minute-averaged reactive power (in kilowatt) \n",
    "5. `Voltage`: Minute-averaged voltage (in volt) \n",
    "6. `Global_intensity`: Household global minute-averaged current intensity (in ampere) \n",
    "7. `Sub_metering_1`: Energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the `kitchen`, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n",
    "8. `Sub_metering_2`: Energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the `laundry room`, containing a washing machine, a tumble-drier, a refrigerator and a light. \n",
    "9. `Sub_metering_3`: Energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an `electric water-heater` and an `air-conditioner`.\n",
    "\n",
    "With the above information as context, now retrieve and read the data.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'household_power_consumption.txt'\n",
    "\n",
    "import os\n",
    "if not os.path.isfile(FILENAME):\n",
    "    !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
    "    !unzip household_power_consumption.zip\n",
    "\n",
    "import pandas as pd\n",
    "raw = pd.read_csv(FILENAME, sep=';', parse_dates=True, na_values=['?'])\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Index of the Time Series to `DatetimeIndex`\n",
    "\n",
    "In order for DeepAR to model seasonalities effectively, the algorithm expects the time index in `DateTime` format. Additionally, all time series in the dataset have to have the same frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "raw['DateTime'] = (raw['Date']+' '+raw['Time']).map(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y %H:%M:%S') )\n",
    "raw.drop(['Date','Time'], axis=1, inplace=True)\n",
    "\n",
    "cols_float = raw.drop('DateTime',axis=1).columns\n",
    "raw[cols_float] = raw[cols_float].apply(lambda x: x.astype(float))\n",
    "raw = raw.set_index('DateTime')\n",
    "\n",
    "print('Percentage of missing data at minute-granularity: %2.2f%%' %(100*raw.isnull().mean().mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR Handles Missing Data\n",
    "\n",
    "1.25% of the data at minute-granularity is missing. \n",
    "\n",
    "Since minute-forecasts are not necessary, you will roll up to hour-granularity. This roll-up incidentally alleviates the missing values in your data. However, DeepAR is able to handle missing data: [DeepAR supports missing values, categorical and time series features, and generalized frequencies](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-deepar-now-supports-missing-values-categorical-and-time-series-features-and-generalized-frequencies/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '2H'\n",
    "df = raw.resample(freq, convention='end').sum()\n",
    "print('Percentage of missing data at hour-granularity:: %2.2f%%' %(100*df.isnull().mean().mean()) )\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition the Data into Training and Test Sets  <a name=\"split\"></a>\n",
    "\n",
    "In this example, you want to train a model that can predict the next **1 week** of electricity time series usage. Since your time series has hourly granularity, 1 week is equivalent to 168 hours.\n",
    "\n",
    "You also need to configure the so-called `context_length`, which determines how much context of the time series the model should take into account when making the prediction, i.e. how many previous points to look at. A typical value to start with is around the same size as the `prediction_length`. In practice, it is often beneficial to use a longer `context_length`. In addition to the `context_length` the model also takes into account the values of the time series at typical seasonal windows. For instance, for hourly data, the model will look at the value of the series 24 hours ago, one week ago, one month ago, etc. Therefore, it is not necessary to make the `context_length` span an entire month if you expect monthly seasonalities in your hourly data.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = int(7*24/int(freq.replace('H','')))\n",
    "context_length = int(7*24/int(freq.replace('H','')))\n",
    "\n",
    "print('Preditction length: %i %s' %(prediction_length, freq))\n",
    "print('Context length: %i %s' %(context_length, freq))\n",
    "print('-->  1 week')\n",
    "\n",
    "n_weeks = 6\n",
    "end_training = df.index[-n_weeks*prediction_length]\n",
    "\n",
    "time_series = []\n",
    "for ts in df.columns:\n",
    "    time_series.append(df[ts])\n",
    "    \n",
    "time_series_training = []\n",
    "for ts in df.columns:\n",
    "    time_series_training.append(df.loc[:end_training][ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Time Series <a name=\"visualize\"></a>\n",
    "\n",
    "Often it is interesting to tune or evaluate the model by looking at error metrics on a hold-out set. For other machine learning tasks such as classification, one typically does this by randomly separating examples into train/test sets. For forecasting it is important to do this train/test split in time rather than by series.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "colors = cmap(np.arange(0,len(cols_float))/len(cols_float))\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,8]);\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(df.loc[:end_training][cols_float[c]], alpha=0.5, color=colors[c], label=cols_float[c]);  \n",
    "plt.legend(loc='center left');\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(df.loc[end_training:][cols_float[c]], alpha=0.25, color=colors[c], label=None);\n",
    "    #plt.plot(df[cols_float[c]], alpha=0.25, color=colors[c], linestyle=':', label=None);\n",
    "plt.axvline(x=end_training, color='k', linestyle=':');\n",
    "plt.text(df.index[int((df.shape[0]-n_weeks*prediction_length)*0.75)], df.max().max()/2, 'Train');\n",
    "plt.text(df.index[df.shape[0]-int(n_weeks*prediction_length/2)], df.max().max()/2, 'Test');\n",
    "plt.xlabel('Time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon S3 Data Formatting <a name=\"s3\"></a>\n",
    "\n",
    "DeepAR supports JSON, gzipped JSON Lines, and Parquet. Use JSON strings for this lab. The following utility functions convert `pandas.Series` objects into the JSON format that DeepAR expects. Use these to write the data to S3.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "encoding = \"utf-8\"\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test.json\", 'wb') as fp:\n",
    "    for ts in time_series:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR: LSTM & Monte Carlo <a name=\"deepar\"></a>\n",
    "\n",
    "Amazon SageMaker DeepAR is a methodology for producing accurate probabilistic forecasts, based on training an auto-regressive recurrent network model on a large number of related time series. DeepAR produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.\n",
    "\n",
    "* The DeepAR algorithm first tailors a `Long Short-Term Memory` (LSTM)-based recurrent neural network architecture to the data. DeepAR then produces probabilistic forecasts in the form of `Monte Carlo` simulation. \n",
    "* `Monte Carlo` samples are empirically generated pseudo-observations that can be used to compute consistent quantile estimates for all sub-ranges in the prediction horizon.\n",
    "* DeepAR also uses item-similarity to handle the `Cold Start` problem, which is to make predictions for items with little or no history at all.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model <a name=\"fit\"></a>\n",
    "\n",
    "Next, configure the container image to be used for the region in which your notebook is running, and define the estimator that will launch the training job.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='DeepAR-forecast-electricity',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set some hyperparameters: \n",
    "\n",
    "\n",
    "| Hyperparameters          | Value                     |\n",
    "|--------------------------|---------------------------|\n",
    "| epochs                   | 100                       |\n",
    "| time granularity         | bi-hourly                 |\n",
    "| domain                   | $\\mathbb{R}^+,\\mathbb{N}$ |\n",
    "| number training examples | 8606                      |\n",
    "| batch size               | 32                        |\n",
    "| learning rate            | $1e-3$                    |\n",
    "| LSTM layers              | 3                         |\n",
    "| LSTM nodes               | 40                        |\n",
    "| likelihood               | gaussian                  |\n",
    "\n",
    "Refer to the documentation for a full description of the available parameters: [https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html). \n",
    "\n",
    "For example, you will elect the Gaussian likelihood model because you have real-valued data. Other likelihood models can also readily be used as long as samples from the distribution can cheaply be obtained and the log-likelihood and its gradients with respect to the parameters can be evaluated. For example:\n",
    "\n",
    "- **Gaussian:** Use for real-valued data.\n",
    "- **Beta:** Use for real-valued targets between 0 and 1 inclusive.\n",
    "- **Negative-binomial:** Use for count data (non-negative integers).\n",
    "- **Student-T:** An alternative for real-valued data that works well for bursty data.\n",
    "- **Deterministic-L1:** A loss function that does not estimate uncertainty and only learns a point forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"100\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now are ready to launch the training job. Amazon SageMaker will start an Amazon EC2 instance, download the data from Amazon S3, start training the model, and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel, as in this example, DeepAR will also calculate accuracy metrics for the trained model on this test dataset. This is done by predicting the last `prediction_length` points of each time series in the test set and comparing this to the actual value of the time series. The computed error metrics will be included as part of the log output.\n",
    "\n",
    "**Note:** The next cell may take **4-6 minutes** to complete. You'll notice at the end that the `billable time` is only around ~150 seconds. This is because customers only pay for consumption of the resources, not the provisioning time.\n",
    "\n",
    "\n",
    "You can follow the progress of the training job, either in the console at [AWS SageMaker Training Jobs page](https://console.aws.amazon.com/sagemaker/home?#/jobs), or in the output generated by the below cell. The cell below will also print training metrics, such as the validation error per epoch, as the job trains. These training metrics are then recorded in the [Amazon CloudWatch Logs](https://console.aws.amazon.com/cloudwatch/home?#logStream:group=/aws/sagemaker/TrainingJobs;prefix=DeepAR-forecast-electricity-2018;streamFilter=typeLogStreamPrefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint and Predictor <a name=\"endpoint\"></a>\n",
    "\n",
    "Now that you have trained a model, use it to perform predictions by deploying it to an endpoint. First, define a [sagemaker.predictor.RealTimeEndpoint](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py) to give your endpoint more functionality and flexibility. This is the `DeepARPredictor` class below, which will allow you to:\n",
    "\n",
    "* Query the endpoint and perform predictions.\n",
    "* Make requests using `pandas.Series` objects rather than raw JSON strings.\n",
    "* Ask for specific quantiles.\n",
    "* Choose the option of returning the Monte Carlo samples or not.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # send only one time series so you only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now ready to deploy your endpoint.\n",
    "\n",
    "**Note:** This endpoint will take approximately **5-8 minutes** to launch. Execute the cell below and use the time to review some important information about the DeepAR model that was just discussed in the previous module presentation. That important recap can be found below the following code cell.\n",
    "\n",
    "It is important to delete an endpoint once it is no longer needed; a cell at the very bottom of this notebook gives the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ....To Read While the Endpoint is Deploying....\n",
    "\n",
    "It is important to elaborate on the DeepAR model's architecture by walking through an example. When interested in quantifying the confidence of the estimates produced, then it's probabilistic forecasts that are wanted. The data you're working with is real-valued, so it is recommended to opt for the Gaussian likelihood:\n",
    "$$\\ell(y_t|\\mu_t,\\sigma_t)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\frac{-(y_t-\\mu_t)^2}{2\\sigma^2}}.$$\n",
    "\n",
    "$\\theta$ represents the `parameters of the likelihood`. In the case of Gaussian, $\\theta_t$ will represent the mean and standard deviation:  $$\\theta_t = \\{\\mu_{t},\\sigma_{t}\\}.$$\n",
    "\n",
    "The neural network’s last hidden layer results in $h_{d,t}$. This $h_{d,t}$ will undergo 1 activation function per likelihood parameter. For example, for the Gaussian likelihood, $h_{d,t}$ is transformed by an affine activation function to get the mean:\n",
    "$$\\mu_{t} = w_{\\mu}^T h_{d,t} + b_{\\mu},$$\n",
    "and then $h$ is transformed by a softplus activation to get the standard deviation:\n",
    "$$\\sigma_t = \\log\\left(1 + \\exp(w_{\\sigma}^T h_{d,t} + b_{\\sigma})\\right).$$\n",
    "\n",
    "The `activation parameters` are the $w_{\\mu},b_{\\mu},w_{\\sigma},b_{\\sigma}$ parameters within the activation functions. The neural network is trained to learn the fixed constants of the activation parameters. Since the $h_{d,t}$ output vary given each time-step's input, this still allows the likelihood parameters to vary over time, and therefore capture dynamic behaviors in the time series data.\n",
    "\n",
    "![DeepAR Training](deepar_training.png)\n",
    "\n",
    "From the above diagram, the <span style=\"color:green\">green</span> input at each time-step is the data point preceding the current time-step’s data, as well as the previous network’s output. For simplicity, on this diagram you aren’t shown covariates which would also be inputs.\n",
    "\n",
    "The LSTM layers are shown in <span style=\"color:red\">red</span>, and the final hidden layer produces the $h_{d,t}$ value, which, as you saw in an earlier slide, will undergo an activation function for each parameter of the specified likelihood. To learn the activation function parameters, the neural network takes the $h_{d,t}$ at time $t$ and the data up until time $t$, and performs Stochastic Gradient Descent (SGD) to yield the activation parameters which maximize the likelihood at time $t$. The <span style=\"color:blue\">blue</span> output layer uses the SGD-optimized activation functions to output the maximum likelihood parameters.\n",
    "\n",
    "This is how DeepAR trains its model to your data input. Now you want DeepAR to give you probabilistic forecasts for the next time-step.\n",
    "\n",
    "![DeepAR Forecast](deepar_forecast.png)\n",
    "\n",
    "The <span style=\"color:magenta\">pink</span> line marks your current point in time, dividing your training data from data not yet seen. For the first input, it can use the data point of the current time. The input will be processed by the trained LSTM layers, and subsequently get activated by the optimized activation functions to output the maximum-likelihood theta parameters at time $t+1$. \n",
    "\n",
    "Now that DeepAR has completed the likelihood with its parameter estimates, DeepAR can simulate `Monte Carlo (MC) samples` from this likelihood and produce an empirical distribution for the predicted datapoint - the probabilistic forecasts shown in <span style=\"color:purple\">purple</span>. The Monte Carlo samples produced at time $t+1$ are used as input for time $t+2$, etc, until the end of the prediction horizon. In the interactive plots below, you'll see how Monte Carlo samples are able to provide us a confidence interval about the point estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions and Plot Results <a name=\"predict\"></a>\n",
    "\n",
    "Once the endpoint finishes deploying, use the previously created `predictor` object. You can see its output of quantile-predictions for the time series Global_active_power.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=df.loc[end_training:, 'Global_active_power'], quantiles=[0.10, 0.5, 0.90], num_samples=100).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare the results with the actual data kept in the test set. To visualize the predictions against the actual targets, we provide the following plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80,\n",
    "    num_samples=100\n",
    "):\n",
    "    print(\"Calling endpoint to generate {} predictions starting from {} ...\".format(target_ts.name, str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # first construct the argument to call your model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": num_samples\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    mccolor = 'blue'\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                #prediction[key].plot(color='light'+mccolor.replace('dark',''), alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    plt.title(target_ts.name.upper(), color='darkred')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=mccolor, alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=mccolor, label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot DeepAR's predictions against the actual targets. The plot below is interactive, so you can explore different confidence intervals, forecast days, and Monte Carlo samples. Take a few minutes to explore this interactive plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "@interact_manual(\n",
    "    series_id=IntSlider(min=0, max=len(time_series_training)-1, value=6, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=0, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    num_samples=IntSlider(min=100, max=1000, value=100, step=500, style=style),\n",
    "    show_samples=Checkbox(value=True),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(series_id, forecast_day, confidence, history_weeks_plot, show_samples, num_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=time_series[series_id],\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * prediction_length,\n",
    "        confidence=confidence,\n",
    "        num_samples=num_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, the Monte Carlo probabilistic forecasts for all seven time series are plotted farther below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(time_series_training)):\n",
    "    plt.figure()\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=time_series[i],\n",
    "        forecast_date=end_training,\n",
    "        show_samples=True,\n",
    "        plot_history= 2 * prediction_length,\n",
    "        confidence=60,\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete endpoint <a name=\"delete\"></a>\n",
    "\n",
    "Remember to delete your Amazon SageMaker endpoint once it is no longer needed. \n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_end = time.time()\n",
    "print('Whole notebook elapsed time (min):', (whole_end-whole_start)/60.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Complete\n",
    "\n",
    "Congratulations! You have completed this lab. To clean up your lab environment, do the following:\n",
    "\n",
    "1. To sign out of the AWS Management Console, click **awsstudent** at the top of the console, and then click **Sign Out**.\n",
    "1. On the Qwiklabs page, click **End**.\n",
    "\n",
    "<div style=\\\"text-align: right\\\"><b>END OF LAB</b></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building a Neural Network with Gluon\n",
    "\n",
    "## Background to Lab\n",
    "\n",
    "In this lab, you will use an Amazon SageMaker-hosted notebook running on an `ml.m4.xlarge` instance and the MXNet kernel to train a neural network using Gluon. You will then perform predictions against this model to identify different items of clothing within the FashionMNIST dataset via Gluon’s `data.vision.datasets` module.\n",
    "\n",
    "### Acknowledgements and Dataset License\n",
    "\n",
    "This lab is based on the [Gluon Crash Course](https://gluon-crash-course.mxnet.io), which is in turn based on content from [Deep Learning - The Straight Dope](https://gluon.mxnet.io/).\n",
    "\n",
    "Notebook material has also been used from [Simon Corston-Oliver's GitHub](https://github.com/simoncorstonoliver/DeepLearningWithMXNetGluon)\n",
    "\n",
    "Below is the licensing agreement for the dataset used in this lab.\n",
    "\n",
    "The MIT License (MIT) Copyright © [2017] Zalando SE, https://tech.zalando.com.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Table of Contents <a name=\"toc\"></a>\n",
    "\n",
    "1. <a href=\"#start\">Getting Started</a>\n",
    "1. <a href=\"#datamanipulation\">Data Manipulation</a>\n",
    "1. <a href=\"#neuralnetwork\">Create a Neural Network</a>\n",
    "1. <a href=\"#differentiation\">Automatic Differentiation with `autograd`</a>\n",
    "1. <a href=\"#train\">Train the Neural Network</a>\n",
    "1. <a href=\"#predict\">Predict with a Pre-Trained Model</a>\n",
    "\n",
    "## Getting Started\n",
    "<a name=\"start\"></a>\n",
    "\n",
    "To complete this lab, carefully move through this notebook, from top to bottom, making sure to read all text instructions/explanations and run each code cell in order. Also be sure to view the code cell outputs. To run each cell, step-by-step in the Jupyter notebook, click within the cell and press **SHIFT + ENTER** or choose **Run** at the top of the page. You will know that a code cell has completed running when you see a number inside the square brackets located to the left of the code cell. Alternatively, **[ ]** indicates the cell has yet to be run, and **[*]** indicates that the cell is still processing.\n",
    "\n",
    "## Data Manipulation\n",
    "<a name=\"datamanipulation\"></a>\n",
    "\n",
    "Now to get started, install the needed utilities and packages for the lab.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet.gluon.utils import download\n",
    "from notebook.services.config import ConfigManager\n",
    "from time import time\n",
    "\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'width': 1000,\n",
    "              'height': 800,\n",
    "              'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix operations form the core computational requirement when working on neural networks and lend themselves well to massive parallel processing operations across GPUs. Indeed, when formulating your problem, you begin with a set of inputs, which may take the form of a matrix. These inputs are manipulated through a series of operations such as multiplication or transformations to ultimately lead to a set of output results. In Python, NumPy is a key scientific computing package allowing manipulation of n-dimensional arrays. In MXNet and Gluon, there is the concept of NDArray, which is very similar to NumPy. This forms the basis of many matrix operations when computing neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.ones(shape=(2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you’ll want to create arrays with values that are sampled randomly. For example, sampling values uniformly between `-1` and `1`. Here you can create the same shape, but with random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nd.random.uniform(low=-1, high=1, shape=(2,3))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with NumPy, the dimensions of each NDArray are accessible by accessing the `.shape` attribute. You can also query its `size`, which is equal to the product of the components of the shape. In addition, `.dtype` tells the data type of the stored values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDArray Metadata\n",
    "(x.shape, x.size, x.dtype, x.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type\n",
    "nd.ones((2,3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context \n",
    "\n",
    "Unlike Numpy, NDArray has a `context` attribute that specifies which device the array resides on. By default, it is `cpu()`. To use the CPU to process the NDArray, set the context attribute to be `ctx=mx.cpu()`. You could replace this with `mx.gpu()` if you wish to use GPU instances. You can define which GPU core to use via `mx.gpu(0)`, which denotes the first core; `mx.gpu(1)` denotes the second and so on. When you call NDArray, the output will tell you where the data is stored. In the below example, this is `<NDArray 2x3 @cpu(0)>`, identifying that the array is stored on the first CPU core.\n",
    "\n",
    "For a CPU, MXNet will allocate data on main memory, and try to use all CPU cores where possible, even if there is more than one CPU socket available. Conversely, if there are multiple GPUs, MXNet needs to specify which GPUs the NDArray will be allocated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.ones((2,3), ctx=mx.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "\n",
    "NDArray supports a large number of standard mathematical operations, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-Wise Manipulation\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponentiation\n",
    "x.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a Matrix's Transpose\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-Matrix Product\n",
    "nd.dot(x, y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 2nd and 3rd Columns\n",
    "y[:, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a Specific Element\n",
    "y[1:2, 0:2] = 4\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Between MXNet NDArrays and NumPy\n",
    "\n",
    "Converting MXNet NDArrays to and from NumPy is rather simple.\n",
    "\n",
    "**Note:** The converted arrays do not share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x.asnumpy()\n",
    "(type(a), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.array(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Neural Network\n",
    "<a name=\"neuralnetwork\"></a>\n",
    "\n",
    "In this section, you will see how to create neural networks using Gluon.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Your First Neural Network Layer\n",
    "\n",
    "Start by creating a dense layer with 2 output units and a ReLu activation function. The `None` value seen in the output is because no data has been passed in at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Dense(2, activation=\"relu\")\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the associated parameters of the dense layer. You can see it has weight and bias attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Next, initialize its weights with the Xavier initialization method. This initializer is designed to keep the scale of gradients roughly the same in all layers. See the [MXNet Initializer documentation](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.initializer.Xavier) for more details on this and other initializers.\n",
    "\n",
    "\n",
    "\n",
    "**Note:** You can safely ignore any warnings generated by the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.initialize(mx.init.Xavier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward with Input `x`\n",
    "\n",
    "Next, do a forward pass with random data. Create a $(3,4)$ shape random input `x` and feed it into the layer to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "x = nd.random.uniform(low=-1, high=1, shape=(N, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = layer(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferred Shape\n",
    "\n",
    "As you can see, the layer's input limit of 2 produced a $(3,2)$ shape output from your $(3,4)$ input. Note that you didn't specify the input size of `layer` before (though you can specify it with the argument `in_units=4`). The system will automatically infer it during the first time data is fed in, creating and initializing the weights. You can access the weight after the first forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Layers into a Neural Network\n",
    "\n",
    "Consider a simple case that a neural network is a chain of layers. During the forward pass, you run layers sequentially one-by-one. The below cell implements a famous neural network called [LeNet](http://yann.lecun.com/exdb/lenet/) through `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    # Add a sequence of layers.\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=6, kernel_size=(5, 5), activation='relu'),\n",
    "\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the neural network. The below output illustrates the network architecture from the bottom up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.viz.plot_network(net(mx.sym.var('data')), \n",
    "                    shape={\"data\":(1, 1, 28, 28)},\n",
    "                    node_attrs={\"shape\":\"oval\",\"fixedsize\":\"False\"},\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Network\n",
    "\n",
    "The usage of `nn.Sequential` is similar to `nn.Dense` (both of them are subclasses of `nn.Block`). The following code shows how to initialize the weights and then run the forward pass. This example uses the default initializer. The input shape is 4 x 1 x 28 x 28 which represents a batch of 4 black and white images (i.e. 1 colour channel) of 28x28 pixels. Passing this through the neural network creates a 4 x 10 shaped output, which represents each of the 4 batches coupled with one of the 10 fashion categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "\n",
    "# Input shape is (batch_size, color_channels, height, width)\n",
    "x = nd.random.uniform(shape=(4, 1, 28, 28))\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Layer\n",
    "\n",
    "You can use `[]` to index a particular layer and access its attributes. For example, first you can view the structure of the neural network via `net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you access the 1st layer's weight and 6th layer's bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"First Conv2D layer weight shape: {}, bias shape: {}\".format(net[0].weight.data().shape, net[5].bias.data().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Neural Network Flexibly\n",
    "\n",
    "In `nn.Sequential`, MXNet will automatically construct the forward function that sequentially executes added layers. Another way to construct a network is with a flexible forward function. This is a custom construct using the `Block` class. \n",
    "\n",
    "Create a subclass of `nn.Block` and implement two methods:\n",
    "\n",
    "- `__init__`: Create the layers.\n",
    "- `forward`: Define the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Run `nn.Block`'s init method\n",
    "        super(MixMLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.features = nn.Sequential()\n",
    "            # Already within a name scope, no need to create\n",
    "            # another scope.\n",
    "            self.features.add(\n",
    "                nn.Dense(3, activation='relu'),\n",
    "                nn.Dense(4, activation='relu')\n",
    "            )\n",
    "            self.output = nn.Dense(5)\n",
    "    def forward(self, x):\n",
    "        y = self.features(x)\n",
    "        print(\"Features\", y)\n",
    "        return self.output(y)\n",
    "\n",
    "net2 = MixMLP()\n",
    "net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Statements\n",
    "\n",
    "In the sequential chaining approach, you can only add instances with `nn.Block` as the base class and then run them in a forward pass. In this example, you use `print` to get the intermediate results and `nd.relu` to apply relu activation. This approach provides a more flexible way to define the forward function.\n",
    "\n",
    "Note that the usage of `net` is similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.random.uniform(shape=(2,2))\n",
    "out = net2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Access\n",
    "\n",
    "Lastly, you can access a particular layer's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2.features[1].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation with `autograd`\n",
    "<a name=\"differentiation\"></a>\n",
    "\n",
    "Models are trained as a function of experience. Usually, getting better means minimizing a loss function. To achieve this goal, you often iteratively compute the gradient of the loss with respect to weights and then update the weights accordingly. While the gradient calculations are straightforward through a chain rule, working it out by hand can be painful for complex models.\n",
    "\n",
    "Before diving deep into the model training, go through how MXNet’s `autograd` package expedites this work by automatically calculating derivatives.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate $f(x) = 2 x^2$ with Respect to Parameter, $x$\n",
    "\n",
    "As an example, assume you are interested in differentiating a function $f(x) = 2 x^2$ with respect to parameter $x$. Start by assigning an initial value of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you compute the gradient of $f(x)$ with respect to $x$, you will need a place to store it. In MXNet, you can tell an NDArray that you plan to store a gradient by invoking its `attach_grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $y=f(x)$\n",
    "\n",
    "Now define the function $y=f(x)$. To let MXNet store $y$, so that you can compute gradients later, you need to put the definition inside an `autograd.record()` scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2 * x**2\n",
    "\n",
    "with autograd.record():\n",
    "    y = f(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation of $y$\n",
    "\n",
    "Invoke back propagation (backprop) by calling `y.backward()`. When $y$ has more than one entry, `y.backward()` is equivalent to `y.sum().backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $y=2x^2$  and $\\frac{dy}{dx} = 4x$\n",
    "\n",
    "Now check if this is the expected output. Note that $y=2x^2$ and $\\frac{dy}{dx} = 4x$, which should be `[[4, 8],[12, 16]]`. Check the automatically computed results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python Control Flows\n",
    "\n",
    "You may want to write dynamic programs where the execution depends on some real-time values. MXNet will record the execution trace and compute the gradient as well.\n",
    "\n",
    "Consider the following function $f$: it doubles the inputs until its `norm` reaches 1000. Then, it selects one element depending on the sum of its elements.\n",
    "\n",
    "$Y=f(X)$\n",
    "- Take a vector `X` of two random numbers in `[-1, 1]`.\n",
    "- `X` is multiplied by `2` until its norm reaches `1000`.\n",
    "- If `X`'s sum is positive, return the 1st element, otherwise return the 2nd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x * 2\n",
    "    while x.norm().asscalar() < 1000:\n",
    "        x = x * 2\n",
    "    # If sum positive\n",
    "    # pick 1st\n",
    "    if x.sum().asscalar() >= 0:\n",
    "        y = x[0]\n",
    "    # else pick 2nd\n",
    "    else:\n",
    "        y = x[1]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the trace and feed in a random value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.random.uniform(-1, 1, shape=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = f(x)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $y=k.x[0]$ or $y=k.x[1]$, hence $\\frac{dy}{dx} =  \\begin{vmatrix} 0 \\\\ k \\end{vmatrix} $ or $ \\begin{vmatrix} k \\\\ 0 \\end{vmatrix}$\n",
    "\n",
    "With $k = 2^n$ where $n$ is the number of times $x$ was multiplied by 2.\n",
    "\n",
    "You know that $y$ is a linear function of $x$, and $y$ is chosen from $x$.\n",
    "\n",
    "Thus, the gradient with respect to $x$ will be either `[y/x[0], 0]` or `[0, y/x[1]]`, depending on which element from $x$ was picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# BREAK\n",
    "\n",
    "Please pause here for a few minutes so we can come back together as a group to review important aspects of what you just accomplished in the above portion of this lab. The instructor will take approximately 5 minutes to review a few of these important aspects prior to you moving on to the final section of this lab.\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "<a name=\"train\"></a>\n",
    "\n",
    "In this section, you will learn how to train the previously defined network with data.\n",
    "\n",
    "### Training Dataset: FashionMNIST\n",
    "\n",
    "The handwritten digit MNIST dataset is one of the most commonly used datasets in deep learning. However, it is too simple to get a 99% accuracy. Here, you will use a similar but slightly more complicated dataset called FashionMNIST. The goal is no longer to classify numbers, but clothing types instead.\n",
    "\n",
    "The dataset can be automatically downloaded through Gluon's `data.vision.datasets` module. The following code downloads the training dataset and shows the first example.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.FashionMNIST(train=True)\n",
    "X, y = mnist_train[0]\n",
    "print('X shape: %s dtype: %s' % (X.shape, X.dtype))\n",
    "print(\"Number of images: %d\" % len(mnist_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example in this dataset is a $28\\times 28$ size grey image, which is presented as NDArray with the shape format of `(height, width, channel)`. The label is a `numpy` scalar.\n",
    "\n",
    "Visualize the first six examples by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = [\n",
    "    't-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "]\n",
    "\n",
    "def plot_images(X, y):\n",
    "    _, figs = plt.subplots(1, X.shape[0], figsize=(15, 15))\n",
    "    for f,x,yi in zip(figs, X,y):\n",
    "        f.imshow(x.reshape((28,28)).asnumpy())\n",
    "        ax = f.axes\n",
    "        ax.set_title(text_labels[int(yi)])\n",
    "        ax.title.set_fontsize(20)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "X, y = mnist_train[0:6]\n",
    "plot_images(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed data into a Gluon model, you need to convert the images to the `(channel, height, weight)` format with a floating-point data type. It can be done by `transforms.ToTensor`. In addition, normalize all pixel values using `transforms.Normalize` with a real mean of `0.13` and variance of `0.31`. Chain these two transforms together and apply it to the first element of the data pair, namely the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.13, 0.31)])\n",
    "\n",
    "mnist_train = mnist_train.transform_first(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FashionMNIST` is a subclass of `gluon.data.Dataset`, which defines how to get the `i`-th example. In order to use it in training, you need to get a (randomized) batch of examples. It can be easily done by `gluon.data.DataLoader`. Here, use four works to process data in parallel, which is often necessary especially for complex data transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Running the code cell below returns `train_data`, an iterator that yields batches of images and label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_data = gluon.data.DataLoader(\n",
    "    mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a validation dataset and data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_valid = gluon.data.vision.FashionMNIST(train=False)\n",
    "\n",
    "valid_data = gluon.data.DataLoader(\n",
    "    mnist_valid.transform_first(transformer),\n",
    "    batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "Reimplement the same LeNet introduced before. One difference here is that you will change the weight initialization method to `Xavier`, which is a popular choice for deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "net.initialize(init=init.Xavier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Besides the neural network, you also need to define the loss function and optimization method for training. Use standard softmax cross entropy loss for classification problems. It first performs softmax on the output to obtain the predicted probability, and then compares the label with the cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "The optimization method selected is the standard stochastic gradient descent with a constant learning rate of `0.1`.\n",
    "\n",
    "The `trainer` is created with all parameters (both weights and gradients) in `net`. Later on, you will only need to call the `step` method to update its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "To evaluate how good your model is, you need to compute both loss and accuracy. Loss relates to the number of errors made for the training and validation sets. Accuracy is a percentage describing how confident the model is in its prediction. To calculate this, sum the number of correctly predicted labels and divide by the total number of predictions. Higher accuracy is preferred. Loss is calculated via a Softmax Cross Entropy function which calculates the distance between predictions and true label. Lower loss is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    acc = (output.argmax(axis=1) == label.astype('float32'))\n",
    "    return acc.mean().asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Now implement the complete training loop. The below should take around 2 minutes to complete on a m4.xlarge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
    "    tic = time()\n",
    "    for data, label in train_data:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean().asscalar()\n",
    "        train_acc += acc(output, label)\n",
    "\n",
    "    # calculate validation accuracy\n",
    "    for data, label in valid_data:\n",
    "        valid_acc += acc(net(data), label)\n",
    "\n",
    "    print(\"Epoch %d: Loss: %.3f, Train acc %.3f, Test acc %.3f, \\\n",
    "Time %.1f sec\" % (\n",
    "        epoch, train_loss/len(train_data),\n",
    "        train_acc/len(train_data),\n",
    "        valid_acc/len(valid_data), time()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "\n",
    "Finally, save the trained parameters onto disk so that they can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Validation accuracy: %.2f\"%(valid_acc/len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('net.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with a Pre-Trained Model\n",
    "<a name=\"predict\"></a>\n",
    "\n",
    "A saved model can be used in multiple places such as to continue training, to fine tune the model, and for prediction. In this section you will learn how to predict new examples using a pretrained model.\n",
    "\n",
    "### Build the Model\n",
    "\n",
    "To start, copy a simple model's definition.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Parameters\n",
    "\n",
    "Load your saved parameters from the file `(net.params)` you saved previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_parameters('net.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Perform the same transformation you used for training, but this time for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.13, 0.31)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to predict the first six images in the validation dataset and store the predictions into `preds` and run through the neural network. Finally, visualize the images and compare the prediction with the truth. Note that in the output, the top label is the actual value (Truth) and the lower label is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_valid = datasets.FashionMNIST(train=False)\n",
    "X, y = mnist_valid[:6]\n",
    "\n",
    "preds = []  \n",
    "for x in X:\n",
    "    x = transformer(x).expand_dims(axis=0)\n",
    "    pred = net(x).argmax(axis=1)\n",
    "    preds.append(pred.astype('int32').asscalar())\n",
    "\n",
    "def plot_images(X, y, preds):\n",
    "    _, figs = plt.subplots(1, 6, figsize=(15, 15))\n",
    "    text_labels = [\n",
    "        't-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "        'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "    ]\n",
    "    for f,x,yi,pyi in zip(figs, X, y, preds):\n",
    "        f.imshow(x.reshape((28,28)).asnumpy())\n",
    "        ax = f.axes\n",
    "        ax.set_title(text_labels[yi]+'\\n'+text_labels[pyi])\n",
    "        ax.title.set_fontsize(20)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "plot_images(X, y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with Models from the Gluon Model Zoo\n",
    "\n",
    "The LeNet trained on FashionMNIST is a good example to start with, but too simple to predict real-life pictures. Instead of training a large-scale model from scratch, [Gluon Model Zoo](https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html) provides multiple, powerful pre-trained models.\n",
    "\n",
    "For example, download and load a pre-trained ResNet-50 V2 model that was trained on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet50_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also download and load the text labels for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://data.mxnet.io/models/imagenet/synset.txt'\n",
    "fname = download(url)\n",
    "with open(fname, 'r') as f:\n",
    "    text_labels = [' '.join(l.split()[1:]) for l in f]\n",
    "    \n",
    "print(*text_labels[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, a dog image was randomly picked from [Wikipedia](https://www.wikipedia.org/) as a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"golden_retriever.jpg\"\n",
    "plt.imshow(plt.imread(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The conventional method of preprocessing ImageNet data includes:\n",
    "\n",
    "1. Resizing the short edge into 256 pixels.\n",
    "1. Performing a center crop to obtain a 224-by-224 image.\n",
    "\n",
    "The following code uses the image processing functions provided in the MXNet [`image` Module](https://mxnet.incubator.apache.org/api/python/image/image.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.imread(fname)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.resize_short(x, 256)\n",
    "x, _ = image.center_crop(x, (224,224))\n",
    "\n",
    "plt.imshow(x.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may already know it is an image of a Golden Retriever (you can also infer it from the image URL).\n",
    "\n",
    "The further data transformation is similar to FashionMNIST except that you subtract the RGB means and divide by the corresponding variances to normalize each color channel.\n",
    "\n",
    "After this, you can recognize the object in the image. Perform an additional softmax activation function on the output to obtain probability scores for the different types of objects, and then print the top-5 recognized objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    data = data.transpose((2,0,1)).expand_dims(axis=0)\n",
    "    rgb_mean = nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1))\n",
    "    rgb_std = nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1))\n",
    "    return (data.astype('float32') / 255. - rgb_mean) / rgb_std\n",
    "\n",
    "output = net(transform(x))\n",
    "pred = output.softmax()\n",
    "\n",
    "idx = pred.topk(k=5).squeeze()\n",
    "for i in idx:\n",
    "    i = int(i.asscalar())\n",
    "    print('With prob = %.5f, it contains %s' % (\n",
    "        pred[0,i].asscalar(), text_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is highly confident the image contains a Golden Retriever.\n",
    "\n",
    "# Lab Complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
